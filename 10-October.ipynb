{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ce1bce6-44f8-4939-be11-62917cb0a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44a7cf74-4c25-40d9-8281-2c100267da26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.1-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /home/safaet/jupyter/jup_notebook/lib/python3.12/site-packages (from xgboost) (2.1.2)\n",
      "Collecting nvidia-nccl-cu12 (from xgboost)\n",
      "  Downloading nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: scipy in /home/safaet/jupyter/jup_notebook/lib/python3.12/site-packages (from xgboost) (1.14.1)\n",
      "Downloading xgboost-2.1.1-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl (199.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.0/199.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nccl-cu12, xgboost\n",
      "Successfully installed nvidia-nccl-cu12-2.23.4 xgboost-2.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec6748-d254-4e9e-963d-95c5fb7c4e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(random_state=42)\n",
    "clf2 = XGBClassifier(random_state=42)\n",
    "clf3 = SVC(probability=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51380dbf-32d8-4cf6-a61d-51e5403884eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "nn_model = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42)\n",
    "xgb_model = XGBClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35af344d-d937-4271-87d8-cca7995a89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "etc_model =  ExtraTreesClassifier()\n",
    "rf_model = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "767b7478-834f-4e32-8f1e-5b0869be6e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = []\n",
    "# models.append(('LogR', LogisticRegression()))\n",
    "# models.append(('SVM', SVC(kernel = 'rbf', probability=True)))\n",
    "# models.append(('RF', RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state=0)))\n",
    "# models.append(('ETC', ExtraTreesClassifier()))\n",
    "# models.append(('XGB', XGBClassifier(use_label_encoder=False, eval_metric='logloss')))\n",
    "\n",
    "# models.append(('gNB', GaussianNB()))\n",
    "# models.append(('DT', DecisionTreeClassifier(criterion = 'entropy', random_state=1)))\n",
    "# models.append(('KNN', KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)))\n",
    "# models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "# models.append(('mNB', MultinomialNB()))\n",
    "# models.append(('ABC', AdaBoostClassifier()))\n",
    "# models.append(('GBC', GradientBoostingClassifier()))\n",
    "\n",
    "# models.append(('BC', BaggingClassifier()))\n",
    "\n",
    "# models.append(('ENS-1', VotingClassifier(\n",
    "#     estimators=[\n",
    "#         ('log_reg', log_reg_model),\n",
    "#         ('gb', gb_model),\n",
    "#         ('nn', nn_model),\n",
    "#         ('xgb', xgb_model)\n",
    "#     ],\n",
    "#     voting='soft'  # Use 'hard' for majority voting or 'soft' for weighted voting\n",
    "# )))\n",
    "\n",
    "# models.append(('ENS-2', VotingClassifier(estimators=[\n",
    "#     ('lr', clf1), ('rf', clf2), ('svc', clf3)\n",
    "# ],voting='soft'\n",
    "# )))\n",
    "\n",
    "models.append(('ENS-1', VotingClassifier(\n",
    "    estimators=[\n",
    "        ('RF', rf_model),\n",
    "        ('ETC', etc_model),\n",
    "        ('XGB', xgb_model)\n",
    "    ],\n",
    "    voting='soft'  # Use 'hard' for majority voting or 'soft' for weighted voting\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42359d7-3e24-4842-add8-05481f08520c",
   "metadata": {},
   "source": [
    "# OverSamplind the data\n",
    "\n",
    "> Add blockquote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56fc06d9-eb89-468a-8c64-bcabba49bf12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def DS_balance_undersampler(X, y):\n",
    "  undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "  X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "  return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "def DS_balance_oversampler(X, y):\n",
    "  oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "  X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "  return X_resampled, y_resampled\n",
    "\n",
    "def DS_balance_SMOTE(X, y):\n",
    "  smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "  X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "  return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c574e67-4b90-44cb-8c38-350ab2aa2415",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ['https://raw.githubusercontent.com/safaet/antimicrobial_resistance/main/Data/Read%20Data/amr_ast_doripenem_PA.csv',\n",
    "          'https://raw.githubusercontent.com/safaet/antimicrobial_resistance/main/Data/Read%20Data/amr_ast_clindamycin_CJ.csv',\n",
    "          'https://raw.githubusercontent.com/safaet/antimicrobial_resistance/main/Data/Read%20Data/amr_ast_doripenem_EcS.csv',\n",
    "          'https://raw.githubusercontent.com/safaet/antimicrobial_resistance/main/Data/Read%20Data/amr_ast_doripenem_KN.csv',\n",
    "          'https://raw.githubusercontent.com/safaet/antimicrobial_resistance/main/Data/Read%20Data/amr_ast_ertapenem_EcS.csv',\n",
    "          'https://raw.githubusercontent.com/safaet/antimicrobial_resistance/main/Data/Read%20Data/amr_ast_ertapenem_KN.csv',\n",
    "          'https://raw.githubusercontent.com/safaet/antimicrobial_resistance/main/Data/Read%20Data/amr_ast_imipenem_EcS.csv',\n",
    "          'https://raw.githubusercontent.com/safaet/antimicrobial_resistance/main/Data/Read%20Data/amr_ast_imipenem_KN.csv',\n",
    "          'https://raw.githubusercontent.com/safaet/antimicrobial_resistance/main/Data/Read%20Data/amr_ast_kanamycin_SE.csv',\n",
    "          'https://raw.githubusercontent.com/safaet/antimicrobial_resistance/main/Data/Read%20Data/amr_ast_meropenem_EcS.csv',\n",
    "          'https://raw.githubusercontent.com/safaet/antimicrobial_resistance/main/Data/Read%20Data/amr_ast_meropenem_KN.csv',\n",
    "          'https://raw.githubusercontent.com/safaet/antimicrobial_resistance/main/Data/Read%20Data/amr_ast_streptomycin_SE.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "451241ca-b073-45d6-aa51-811c7f4cbc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doripenem\n",
      "amr_ast_doripenem_PA.csv\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0].split('/')[-1].split('_')[-2])\n",
    "print(dataset[0].split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2fc8fb8c-7ded-4861-83fd-1703bc0b2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    df = pd.read_csv(dataset[i])\n",
    "    file_out = '/home/safaet/jupyter/Thesis_work/antimicrobial_resistance/AllResults/ResultsPlainEnsemble/' + dataset[i].split('/')[-1]\n",
    "    \n",
    "    if 'sum' not in df.columns:\n",
    "      # slice dataset and store in df2\n",
    "      df2 = df.iloc[:, 1:-1]\n",
    "      df['sum'] = df2.sum(axis=1)\n",
    "    y = df.pop(dataset[i].split('/')[-1].split('_')[-2])\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_column = scaler.fit_transform(df['sum'].values.reshape(-1, 1))\n",
    "    df['sum'] = scaled_column\n",
    "    X = df.iloc[:,1:]\n",
    "    \n",
    "    mode = []\n",
    "    tr_precision=[]\n",
    "    tr_recall = []\n",
    "    tr_f1 = []\n",
    "    tr_accu = []\n",
    "    tr_cv = []\n",
    "    \n",
    "    te_precision=[]\n",
    "    te_recall = []\n",
    "    te_f1 = []\n",
    "    te_accu = []\n",
    "    te_cv = []\n",
    "    \n",
    "    \n",
    "    for k in range(3):\n",
    "    \n",
    "        if (k == 0):\n",
    "            X, y = DS_balance_oversampler(X, y)\n",
    "        \n",
    "        elif (k == 1):\n",
    "            X, y = DS_balance_undersampler(X, y)\n",
    "        \n",
    "        elif (k == 2):\n",
    "            X, y = DS_balance_SMOTE(X, y)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "        for name, model in models:\n",
    "        \n",
    "            model = model.fit(X_train, y_train)\n",
    "            mode.append(name)\n",
    "        \n",
    "            Y_train_pred = model.predict(X_train)\n",
    "            Tr_precision = precision_score(y_train, Y_train_pred, average='macro').round(3)\n",
    "            tr_precision.append(Tr_precision)\n",
    "        \n",
    "            Tr_recall = recall_score(y_train, Y_train_pred, average='macro').round(3)\n",
    "            tr_recall.append(Tr_recall)\n",
    "        \n",
    "            Tr_f1 = f1_score (y_train, Y_train_pred, average='macro').round(3)\n",
    "            tr_f1.append(Tr_f1)\n",
    "        \n",
    "            accuracy = round(model.score(X_train, y_train),3)\n",
    "            tr_accu.append(accuracy)\n",
    "        \n",
    "            cv_score = cross_val_score(model, X,y, cv=6)\n",
    "            mean_accuracy = sum(cv_score)/len(cv_score)\n",
    "            mean_accuracy = round(mean_accuracy, 2)\n",
    "            tr_cv.append(mean_accuracy)\n",
    "        \n",
    "        \n",
    "            Y_test_pred = model.predict(X_test)\n",
    "            report = sklearn.metrics.classification_report(y_test, Y_test_pred)\n",
    "            Te_precision = precision_score(y_test, Y_test_pred, average='macro').round(3)\n",
    "            te_precision.append(Te_precision)\n",
    "        \n",
    "            Te_recall = recall_score(y_test, Y_test_pred, average='macro').round(3)\n",
    "            te_recall.append(Te_recall)\n",
    "        \n",
    "            Te_f1 = f1_score (y_test, Y_test_pred, average='macro').round(3)\n",
    "            te_f1.append(Te_f1)\n",
    "        \n",
    "            accuracy = round(model.score(X_test, y_test),3)\n",
    "            te_accu.append(accuracy)\n",
    "        \n",
    "            cv_score = cross_val_score(model, X,y, cv=6)\n",
    "            mean_accuracy = sum(cv_score)/len(cv_score)\n",
    "            mean_accuracy = round(mean_accuracy, 2)\n",
    "            te_cv.append(mean_accuracy)\n",
    "        \n",
    "            # dict = {'classifier': name, f'tr_precision{i+1}': Tr_precision, f'tr_recall{i+1}': Tr_recall, f'tr_f1 {i+1}':Tr_f1}\n",
    "        \n",
    "            # df = pd.DataFrame(dict)\n",
    "        \n",
    "            # print('Model_Name: ', model)\n",
    "            # print('Tr_precision: ', Tr_precision)\n",
    "            # print('Tr_recall: ', Tr_recall)\n",
    "            # print('Tr_f1: ', Tr_f1)\n",
    "            # print(\"--------------\")\n",
    "        \n",
    "            # i+=1\n",
    "        \n",
    "        dict = {'classifier': mode, 'tr_precision': tr_precision, 'tr_recall': tr_recall, 'tr_f1 ': tr_f1,'tr_accuracy': tr_accu,\n",
    "            'te_precision': te_precision, 'te_recall': te_recall, 'te_f1 ': te_f1, 'te_accuracy': te_accu}\n",
    "        \n",
    "        df = pd.DataFrame(dict)\n",
    "        \n",
    "        df.to_csv(file_out, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d63b41-75f5-4159-8ccf-59e98f884978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
