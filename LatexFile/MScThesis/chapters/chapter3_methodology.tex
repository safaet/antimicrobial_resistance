\chapter{Methodology}

\section{Overview}
This chapter presents a comprehensive methodology for predicting Antimicrobial Resistance (AMR) using machine learning techniques applied to resistance gene profiles. The proposed approach demonstrates that resistance gene-based prediction can achieve performance comparable to, or exceeding, whole-genome sequence (WGS)-based methods while maintaining computational efficiency and biological interpretability.

The methodology encompasses six key stages:
\begin{enumerate}
  \item Dataset Collection and Preprocessing
  \item Feature Engineering
  \item Feature Selection
  \item Class Imbalance Handling
  \item Model Training and Ensemble Construction
  \item Model Interpretability and Explainability
\end{enumerate}

The pipeline addresses specific challenges of AMR genomic data, such as high dimensionality, sparsity of binary gene matrices, and class imbalance. Figure~\ref{fig:workflow} illustrates the complete workflow of the proposed methodology.

\section{Dataset Collection and Preprocessing}
\subsection{Data Source}
The study utilized antimicrobial susceptibility testing (AST) data and genomic profiles from the NCBI Pathogen Detection Database. Resistance gene annotations were obtained via the AMRFinderPlus pipeline. Twelve datasets were compiled:
\begin{itemize}
  \item \textit{Klebsiella pneumoniae}: Doripenem, Ertapenem, Imipenem, Meropenem
  \item \textit{Escherichia coli} and \textit{Shigella}: Doripenem, Ertapenem, Imipenem, Meropenem
  \item \textit{Pseudomonas aeruginosa}: Doripenem
  \item \textit{Salmonella enterica}: Streptomycin, Kanamycin
  \item \textit{Campylobacter jejuni}: Clindamycin
\end{itemize}

\subsection{Binary Gene Matrix Construction}
Each isolate was represented as a binary gene presence vector $\mathbf{x}_i \in \{0, 1\}^n$. Detection completeness metadata (e.g., COMPLETE, PARTIAL) was encoded as additional binary features. The label $y_i \in \{0,1\}$ denotes resistance status.

\section{Feature Engineering}
\subsection{Resistance Gene Load Score (R-Score)}
The R-Score is computed as:
\[ \text{R-Score}_i = \sum_{j=1}^n x_{ij} \]
It is then normalized using min-max scaling:
\[ \text{R-Score}'_i = \frac{\text{R-Score}_i - \min(\text{R-Score})}{\max(\text{R-Score}) - \min(\text{R-Score})} \]

\subsection{Justification and Impact Analysis}
Inclusion of R-Score improved model performance and provided biologically interpretable separation between resistant and susceptible classes.

\section{Feature Selection}
\subsection{Stage 1: ANOVA F-test}
Univariate statistical filtering was performed using ANOVA F-statistics. Features with $p \leq 0.30$ were retained.

\subsection{Stage 2: XGBoost Importance}
An XGBoost model (400 trees, max depth 6) was used to compute feature importances. Features accounting for 85\% cumulative importance were selected.

\subsection{Final Feature Set}
Selected features were combined as:
\[ S_{\text{final}} = S_{\text{ANOVA}} \cup S_{\text{XGB}} \]

\section{Class Imbalance Handling}
\subsection{Resampling Strategy}
The study compared several strategies (SMOTE, ADASYN, etc.) and selected SMOTETomek based on classification performance and biological plausibility.

\section{Model Training and Ensemble Construction}
\subsection{Base Models}
Evaluated models included:
\begin{itemize}
  \item Logistic Regression (LogR)
  \item Support Vector Machine (SVM)
  \item Decision Tree (DT)
  \item Random Forest (RF)
  \item XGBoost (XGB)
\end{itemize}

\subsection{R-Blend Ensemble}
R-Blend is a soft-voting ensemble combining DT, LogR, and XGB with weights [1.0, 1.5, 1.0]:
\[ P(y = c | \mathbf{x}) = \frac{\sum_i w_i \cdot P_i(y=c | \mathbf{x})}{\sum_i w_i} \]

\section{Evaluation Metrics}
The following metrics were used:
\begin{itemize}
  \item Accuracy
  \item Precision
  \item Recall
  \item F1-Score
  \item AUROC
  \item AUPRC
\end{itemize}

\section{Model Interpretability and Explainability}
\subsection{Permutation Importance}
Assesses drop in performance when shuffling feature values.

\subsection{SHAP Values}
Computed using TreeExplainer (for DT/XGB) and LinearExplainer (for LogR):
\[ f(\mathbf{x}) = \phi_0 + \sum_j \phi_j(x_j) \]

\subsection{Ensemble SHAP Aggregation}
\[ \phi^{\text{ensemble}}_j(\mathbf{x}) = \frac{\sum_i w_i \cdot \phi_j^{(i)}(\mathbf{x})}{\sum_i w_i} \]

\section{Experimental Design and Validation}
\begin{itemize}
  \item Ablation studies on R-Score, resampling, and ensemble impact
  \item Validation across 12 datasets with mean \& std metrics
  \item Comparison with published methods (Her \& Wu, Yasir et al.)
\end{itemize}

\section{Implementation}
Code was implemented in Python 3 using scikit-learn, XGBoost, imbalanced-learn, SHAP, pandas, and matplotlib. Experiments were run in Google Colab with GPU acceleration.