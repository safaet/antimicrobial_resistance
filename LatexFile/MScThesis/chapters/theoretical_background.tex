\chapter{THEORETICAL BACKGROUND}

\section{Overview}

Antimicrobial Resistance (AMR) prediction from genomic data represents a critical intersection of machine learning and computational biology. This chapter provides the theoretical foundation for machine learning techniques, feature engineering, ensemble learning, and interpretability frameworks employed in this research for predicting AMR from resistance gene profiles.

\section{Machine Learning Fundamentals}

\subsection{Supervised Learning}

Supervised learning learns a mapping function $f: X \rightarrow Y$ from labeled data $D = \{(x_1, y_1), \ldots, (x_n, y_n)\}$, where $x_i$ is the feature vector and $y_i \in \{0, 1\}$ is the class label (1 = resistant, 0 = susceptible). The dataset is partitioned into training and testing sets using stratified splitting to preserve class proportions. Stratified $k$-fold cross-validation assesses performance by training $k$ times on different partitions, averaging results across iterations.

\section{Feature Engineering and Selection}

\subsection{Feature Engineering}

Feature engineering transforms raw data to improve model performance. Min-Max normalization scales features to $[0, 1]$: $x' = \frac{x - \min(x)}{\max(x) - \min(x)}$. Derived features capture domain knowledge through aggregation (sum, mean, count), ratios, interactions, or binning.

\subsection{Feature Selection}

Feature selection addresses dimensionality, computational efficiency, interpretability, and noise reduction. This research employs a hybrid approach combining filter and embedded methods.

\textbf{ANOVA F-test}: Evaluates features independently using the F-statistic: $F = \frac{\text{Between-group variability}}{\text{Within-group variability}}$. Higher F-statistics indicate greater class separation with statistical significance.

\textbf{XGBoost Gain-Based Importance}: Measures average loss function improvement: $\text{Importance}(\text{feature}) = \sum (\text{Gain from splits})$. Features are ranked and selected until cumulative importance reaches a threshold (e.g., 85\%).

\textbf{Hybrid Strategy}: Combining methods (union approach) leverages statistical univariate relationships and multivariate interactions for comprehensive coverage.

\section{Class Imbalance Handling}

\subsection{The Imbalance Problem}

Class imbalance occurs when the majority class significantly outnumbers the minority class. Imbalance ratio = $\frac{\text{Majority samples}}{\text{Minority samples}}$. Ratios $>1.5:1$ are imbalanced, causing models to bias toward the majority class and poorly predict the critical minority class.

\subsection{SMOTE and SMOTE-Tomek}

\textbf{SMOTE (Synthetic Minority Over-sampling Technique)}: Generates synthetic minority samples by interpolating between existing instances. For sample $x$ with nearest neighbor $x_{nn}$: $x_{\text{synthetic}} = x + \lambda \times (x_{nn} - x)$, where $\lambda \in [0, 1]$. SMOTE creates diverse samples but may generate unrealistic instances in sparse spaces.

\textbf{SMOTE-Tomek}: Combines SMOTE with Tomek links removal. A Tomek link is a pair $(x_i, x_j)$ from different classes that are mutual nearest neighbors. The process: (1) Apply SMOTE, (2) Identify Tomek links, (3) Remove linked samples. This balances classes while improving boundary clarity, removing noisy samples, and reducing overfitting—particularly effective for high-dimensional, sparse data.

\section{Classification Algorithms}

\subsection{Logistic Regression}

Models binary outcome probability using the sigmoid function: $P(y = 1 \mid x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \cdots + \beta_n x_n)}}$. Creates linear decision boundaries, is computationally efficient, produces calibrated probabilities, and offers interpretable coefficients but cannot capture complex non-linear patterns.

\subsection{Support Vector Machine (SVM)}

Finds the optimal separating hyperplane maximizing margin: $f(x) = \text{sign}(w \cdot x + b)$. The kernel trick maps inputs to higher dimensions for non-linear separation. The RBF kernel $K(x, x') = \exp(-\gamma \|x - x'\|^2)$ handles non-linearity with tunable smoothness. Effective in high-dimensional spaces but computationally expensive for large datasets.

\subsection{Decision Tree}

Recursively partitions data using impurity measures. Gini impurity: $\text{Gini}(S) = 1 - \sum p_i^2$. Entropy: $\text{Entropy}(S) = -\sum p_i \times \log_2(p_i)$. Trees are interpretable, require minimal preprocessing, and capture non-linear relationships but are prone to overfitting and instability.

\subsection{Random Forest}

Ensemble of decision trees using bootstrap sampling and random feature selection. Algorithm: (1) Create $B$ bootstrap samples, (2) For each sample, grow a tree selecting $m$ random features at each split, (3) Aggregate predictions via majority voting. Feature importance: $\text{Importance} = \frac{1}{B} \sum \text{Decrease in impurity}$. Reduces overfitting, provides robust importance estimates, handles high-dimensional data well but is less interpretable than single trees.

\subsection{XGBoost (eXtreme Gradient Boosting)}

Sequential tree ensemble where each tree corrects previous errors. Optimizes regularized objective: $\text{Obj} = \sum L(y_i, \hat{y}_i) + \sum \Omega(f_t)$, where $\Omega(f_t) = \gamma T + \frac{\lambda}{2}\|w\|^2$ penalizes complexity ($T$ = leaves, $w$ = weights). Key features: L1/L2 regularization, tree pruning, missing value handling, column/row subsampling. Important hyperparameters: n\_estimators, max\_depth, learning\_rate, subsample, colsample\_bytree, gamma, lambda. Achieves state-of-the-art performance with built-in regularization but requires careful tuning.

\section{Ensemble Learning}

\subsection{Ensemble Principles}

Combines multiple models to reduce variance (averaging predictions), reduce bias (capturing diverse patterns), and improve robustness. Effectiveness requires diversity—models making different errors through different algorithms, training subsets, feature subsets, or hyperparameters.

\subsection{Weighted Soft Voting}

Predicts the class with highest average probability: $\hat{y} = \arg\max \sum w_i \times P_i(y = c \mid x)$, where $w_i$ are importance weights ($\sum w_i = 1.0$) based on validation performance (e.g., F1-score). Leverages probability confidence, assigns greater influence to better models, and combines diverse algorithm strengths.

\section{Model Evaluation Metrics}

\subsection{Confusion Matrix and Basic Metrics}

The confusion matrix contains True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).

\textbf{Accuracy} = $\frac{TP + TN}{TP + TN + FP + FN}$ measures overall correctness but misleads with imbalanced data.

\textbf{Precision} = $\frac{TP}{TP + FP}$ measures positive prediction correctness (important when false positives are costly).

\textbf{Recall} = $\frac{TP}{TP + FN}$ measures actual positive identification (critical when missing positives is costly).

\textbf{F1-Score} = $2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$ harmonically balances both, penalizing extreme values and proving robust to imbalance.

\subsection{ROC and Precision-Recall Curves}

\textbf{AUROC (Area Under ROC Curve)}: Plots True Positive Rate vs. False Positive Rate across thresholds. Represents the probability that a randomly chosen positive ranks higher than a negative. Provides threshold-independent evaluation but may be overly optimistic for imbalanced data.

\textbf{AUPRC (Area Under Precision-Recall Curve)}: Plots Precision vs. Recall across thresholds. Focuses on positive class performance, making it more informative than AUROC for imbalanced datasets where the minority class is critical.

\section{Model Interpretability}

\subsection{Importance of Interpretability}

Interpretability enables trust and adoption, debugging and validation, regulatory compliance, and scientific insight. Models have intrinsic interpretability (linear models, trees) or require post-hoc methods (SHAP, permutation importance).

\subsection{Permutation Feature Importance}

Measures performance degradation when a feature is randomly shuffled, breaking its relationship with the target. Algorithm: (1) Compute baseline performance, (2) For each feature, permute values and compute performance, (3) Importance = Baseline - Permuted performance. Model-agnostic, considers features contextually, but can be unreliable with correlated features.

\subsection{SHAP (SHapley Additive exPlanations)}

Assigns each feature a contribution value based on Shapley values from game theory. The SHAP value $\phi_j$ for feature $j$:
\begin{equation}
\phi_j = \sum_{S \subseteq F \setminus \{j\}} \frac{|S|!(M - |S| - 1)!}{M!} \times [f(S \cup \{j\}) - f(S)]
\end{equation}

SHAP satisfies local accuracy ($f(x) = \phi_0 + \sum \phi_j$), consistency, and additivity. TreeExplainer efficiently computes exact values for tree models.

\textbf{Global Explanations}: Mean absolute SHAP values indicate overall feature importance.

\textbf{Local Explanations}: Instance-level SHAP values show which features contributed to specific predictions.

\textbf{Ensemble SHAP}: For weighted voting with weights $w_i$: $\phi_{j,\text{ensemble}} = \frac{\sum w_i \times \phi_{j,i}}{\sum w_i}$.

\section{Hyperparameter Optimization}

\subsection{Grid Search with Cross-Validation}

Hyperparameters control the learning process (learning rate, tree depth, regularization). Grid search exhaustively evaluates predefined combinations using cross-validation: (1) Train with cross-validation, (2) Compute average performance, (3) Select best combination. Guaranteed to find optimal grid combination and parallelizable but computationally expensive.

\subsection{Cross-Validation Protocol}

Ensures unbiased selection: (1) Split data into train and test sets, (2) Use cross-validation on training set for hyperparameter selection, (3) Train final model with selected hyperparameters, (4) Evaluate on held-out test set. Prevents overfitting to test set performance.

\section{Data Leakage Prevention}

\subsection{Definition and Sources}

Data leakage occurs when test set information influences training, causing overly optimistic estimates. Sources include: (1) Preprocessing applied before splitting (training "sees" test data), (2) Resampling applied to both sets (synthetic test samples use training information).

\subsection{Correct Protocol}

(1) Split data (stratified), (2) Fit preprocessing on training data only, (3) Apply preprocessing to both sets, (4) Apply resampling only to training set, (5) Train on preprocessed, resampled training set, (6) Evaluate on preprocessed (not resampled) test set. Ensures test set remains unseen and represents real-world conditions.

\section{Binary Classification Formulation}

\subsection{Problem Definition}

Binary classification assigns instances to one of two classes. Input: $x \in \mathbb{R}^n$, Output: $y \in \{0, 1\}$, Objective: Learn $f: \mathbb{R}^n \rightarrow \{0, 1\}$ minimizing prediction error.

\subsection{Decision Threshold and Calibration}

Classifiers output probability $P(y = 1 \mid x)$. Predicted class determined by threshold $\tau$: $\hat{y} = 1$ if $P(y = 1 \mid x) \geq \tau$, else 0. Default $\tau = 0.5$, adjustable based on class imbalance and cost trade-offs.

Well-calibrated classifiers produce probabilities reflecting true likelihood. Calibration methods: Platt Scaling (logistic regression on outputs), Isotonic Regression (non-parametric monotonic function). Logistic Regression naturally produces calibrated probabilities; tree models may require calibration.

\section{Chapter Summary}

This chapter established the theoretical foundation for AMR prediction, covering supervised learning, feature engineering and selection (hybrid ANOVA-XGBoost), class imbalance handling (SMOTE-Tomek), classification algorithms (Logistic Regression, SVM, Decision Tree, Random Forest, XGBoost), weighted soft voting ensembles, evaluation metrics (accuracy, precision, recall, F1-score, AUROC, AUPRC), model interpretability (permutation importance, SHAP), hyperparameter optimization, and data leakage prevention. These concepts form the methodological foundation for the subsequent research.
\section{Chapter Summary}

This chapter established the theoretical foundation for AMR prediction, covering supervised learning, feature engineering and selection (hybrid ANOVA-XGBoost), class imbalance handling (SMOTE-Tomek), classification algorithms (Logistic Regression, SVM, Decision Tree, Random Forest, XGBoost), weighted soft voting ensembles, evaluation metrics (accuracy, precision, recall, F1-score, AUROC, AUPRC), model interpretability (permutation importance, SHAP), hyperparameter optimization, and data leakage prevention. These concepts form the methodological foundation for the subsequent research.